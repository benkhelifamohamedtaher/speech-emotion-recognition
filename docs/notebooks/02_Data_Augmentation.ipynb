{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd0a Audio Data Augmentation for Emotion Recognition\n",
        "\n",
        "This notebook explores various data augmentation techniques for audio signals, specifically tailored for speech emotion recognition. Data augmentation is crucial for improving model generalization and performance, especially for deep learning models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "Data augmentation helps to:\n",
        "\n",
        "1. **Increase dataset size**: Generate more training examples from existing data\n",
        "2. **Improve generalization**: Make the model robust to variations in input\n",
        "3. **Reduce overfitting**: Prevent the model from memorizing training examples\n",
        "4. **Handle class imbalance**: Generate more samples for underrepresented classes\n",
        "\n",
        "For speech emotion recognition, we need to be careful about which augmentations to apply, as some transformations may alter emotional content (e.g., heavy pitch shifting might change perceived emotion).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "import torch\n",
        "import torchaudio\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Audio Augmentation Techniques\n",
        "\n",
        "Let's implement and visualize various audio augmentation techniques:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample audio file (assuming we have the RAVDESS dataset)\n",
        "def load_sample_audio(emotion='angry'):\n",
        "    \"\"\"Load a sample audio file for the given emotion\"\"\"\n",
        "    # This is a placeholder function. In real use, you would load from your dataset\n",
        "    try:\n",
        "        # Try to load from processed dataset\n",
        "        emotion_path = f\"../processed_dataset/train/{emotion}\"\n",
        "        if os.path.exists(emotion_path):\n",
        "            files = [f for f in os.listdir(emotion_path) if f.endswith('.wav')]\n",
        "            if files:\n",
        "                file_path = os.path.join(emotion_path, files[0])\n",
        "                waveform, sample_rate = librosa.load(file_path, sr=22050)\n",
        "                return waveform, sample_rate, file_path\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # If no file found, generate dummy audio\n",
        "    print(\"No sample file found, generating dummy audio\")\n",
        "    sample_rate = 22050\n",
        "    duration = 3  # seconds\n",
        "    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
        "    waveform = 0.5 * np.sin(2 * np.pi * 440 * t)  # 440 Hz sine wave\n",
        "    return waveform, sample_rate, \"dummy_audio.wav\"\n",
        "\n",
        "# Function to visualize waveform and spectrogram\n",
        "def visualize_audio(waveform, sample_rate, title=\"Audio\"):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # Plot waveform\n",
        "    plt.subplot(2, 1, 1)\n",
        "    librosa.display.waveshow(waveform, sr=sample_rate)\n",
        "    plt.title(f'{title} - Waveform')\n",
        "    \n",
        "    # Plot spectrogram\n",
        "    plt.subplot(2, 1, 2)\n",
        "    D = librosa.amplitude_to_db(np.abs(librosa.stft(waveform)), ref=np.max)\n",
        "    librosa.display.specshow(D, y_axis='log', x_axis='time', sr=sample_rate)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(f'{title} - Spectrogram')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Play audio\n",
        "    return ipd.Audio(waveform, rate=sample_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Time Domain Augmentations\n",
        "\n",
        "These augmentations modify the signal in the time domain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1 Time Shifting\n",
        "def time_shift(waveform, sample_rate, shift_pct=0.2):\n",
        "    \"\"\"Shift the audio in time (left or right) by a random amount\"\"\"\n",
        "    shift = int(sample_rate * shift_pct * np.random.uniform(-1, 1))\n",
        "    if shift > 0:\n",
        "        # Shift right: insert silence at the beginning\n",
        "        shifted = np.pad(waveform, (shift, 0), mode='constant')[:len(waveform)]\n",
        "    else:\n",
        "        # Shift left: insert silence at the end\n",
        "        shifted = np.pad(waveform, (0, -shift), mode='constant')[:-shift if shift else None]\n",
        "    return shifted\n",
        "\n",
        "# 1.2 Time Stretching\n",
        "def time_stretch(waveform, rate=1.2):\n",
        "    \"\"\"Stretch the audio by a rate factor\"\"\"\n",
        "    return librosa.effects.time_stretch(waveform, rate=rate)\n",
        "\n",
        "# 1.3 Adding Noise\n",
        "def add_noise(waveform, noise_level=0.005):\n",
        "    \"\"\"Add random noise to the audio signal\"\"\"\n",
        "    noise = np.random.normal(0, noise_level, len(waveform))\n",
        "    noisy_signal = waveform + noise\n",
        "    return noisy_signal\n",
        "\n",
        "# 1.4 Clipping\n",
        "def clip_audio(waveform, clip_factor=0.8):\n",
        "    \"\"\"Clip the audio signal\"\"\"\n",
        "    threshold = clip_factor * np.max(np.abs(waveform))\n",
        "    return np.clip(waveform, -threshold, threshold)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Frequency Domain Augmentations\n",
        "\n",
        "These augmentations modify the signal in the frequency domain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 Pitch Shifting\n",
        "def pitch_shift(waveform, sample_rate, n_steps=2):\n",
        "    \"\"\"Shift the pitch up or down by n_steps\"\"\"\n",
        "    return librosa.effects.pitch_shift(waveform, sr=sample_rate, n_steps=n_steps)\n",
        "\n",
        "# 2.2 Frequency Masking (for spectrograms)\n",
        "def frequency_mask(spectrogram, max_mask_width=10, num_masks=1):\n",
        "    \"\"\"Apply frequency masking to a spectrogram\"\"\"\n",
        "    spec = spectrogram.copy()\n",
        "    freq_dim = spec.shape[0]\n",
        "    \n",
        "    for _ in range(num_masks):\n",
        "        f_width = np.random.randint(1, max_mask_width)\n",
        "        f_start = np.random.randint(0, freq_dim - f_width)\n",
        "        spec[f_start:f_start + f_width, :] = 0\n",
        "    \n",
        "    return spec\n",
        "\n",
        "# 2.3. Time Masking (for spectrograms)\n",
        "def time_mask(spectrogram, max_mask_width=20, num_masks=1):\n",
        "    \"\"\"Apply time masking to a spectrogram\"\"\"\n",
        "    spec = spectrogram.copy()\n",
        "    time_dim = spec.shape[1]\n",
        "    \n",
        "    for _ in range(num_masks):\n",
        "        t_width = np.random.randint(1, max_mask_width)\n",
        "        t_start = np.random.randint(0, time_dim - t_width)\n",
        "        spec[:, t_start:t_start + t_width] = 0\n",
        "    \n",
        "    return spec\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Combined Augmentations\n",
        "\n",
        "More complex augmentations combine multiple techniques.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 Room Reverb Simulation\n",
        "def add_reverb(waveform, sample_rate, reverberance=50, damping=50, room_scale=50):\n",
        "    \"\"\"\n",
        "    Simulate room reverb effect. This is a simplified version.\n",
        "    For more accurate reverb, consider using libraries like pyroomacoustics.\n",
        "    \"\"\"\n",
        "    # Convert parameters to values between 0 and 1\n",
        "    reverberance = reverberance / 100\n",
        "    damping = damping / 100\n",
        "    room_scale = room_scale / 100\n",
        "    \n",
        "    # Simple reverb simulation using convolution with decaying impulse response\n",
        "    impulse_length = int(sample_rate * room_scale * 0.5)\n",
        "    impulse = np.exp(-np.arange(impulse_length) / (impulse_length * damping))\n",
        "    reverb = np.convolve(waveform, impulse, mode='full')[:len(waveform)]\n",
        "    \n",
        "    # Mix original and reverbed signal\n",
        "    mixture = (1 - reverberance) * waveform + reverberance * reverb\n",
        "    return mixture / np.max(np.abs(mixture))  # Normalize\n",
        "\n",
        "# 3.2 Filtering (Bandpass, Highpass, Lowpass)\n",
        "def apply_filter(waveform, sample_rate, filter_type='bandpass', cutoff_low=500, cutoff_high=3000):\n",
        "    \"\"\"Apply a filter to the audio\"\"\"\n",
        "    from scipy import signal\n",
        "    \n",
        "    nyquist = sample_rate / 2\n",
        "    if filter_type == 'lowpass':\n",
        "        b, a = signal.butter(4, cutoff_high / nyquist, btype='lowpass')\n",
        "    elif filter_type == 'highpass':\n",
        "        b, a = signal.butter(4, cutoff_low / nyquist, btype='highpass')\n",
        "    else:  # bandpass\n",
        "        b, a = signal.butter(4, [cutoff_low / nyquist, cutoff_high / nyquist], btype='band')\n",
        "    \n",
        "    return signal.filtfilt(b, a, waveform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demonstration of Augmentation Techniques\n",
        "\n",
        "Let's visualize and listen to various augmentations applied to a sample audio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample audio file\n",
        "waveform, sample_rate, _ = load_sample_audio(emotion='angry')\n",
        "\n",
        "# Visualize and play the original audio\n",
        "print(\"Original Audio:\")\n",
        "visualize_audio(waveform, sample_rate, \"Original\")\n",
        "\n",
        "# Apply and visualize different augmentations\n",
        "\n",
        "# 1. Time Shifting\n",
        "print(\"\\nTime Shifted Audio:\")\n",
        "shifted = time_shift(waveform, sample_rate, shift_pct=0.2)\n",
        "visualize_audio(shifted, sample_rate, \"Time Shifted\")\n",
        "\n",
        "# 2. Time Stretching\n",
        "print(\"\\nTime Stretched Audio (Slower):\")\n",
        "stretched = time_stretch(waveform, rate=0.8)  # slower\n",
        "visualize_audio(stretched, sample_rate, \"Time Stretched (Slower)\")\n",
        "\n",
        "print(\"\\nTime Stretched Audio (Faster):\")\n",
        "stretched_fast = time_stretch(waveform, rate=1.2)  # faster\n",
        "visualize_audio(stretched_fast, sample_rate, \"Time Stretched (Faster)\")\n",
        "\n",
        "# 3. Adding Noise\n",
        "print(\"\\nNoisy Audio:\")\n",
        "noisy = add_noise(waveform, noise_level=0.01)\n",
        "visualize_audio(noisy, sample_rate, \"Noisy\")\n",
        "\n",
        "# 4. Pitch Shifting\n",
        "print(\"\\nPitch Shifted Audio (Higher):\")\n",
        "pitched_up = pitch_shift(waveform, sample_rate, n_steps=2)\n",
        "visualize_audio(pitched_up, sample_rate, \"Pitch Shifted (Higher)\")\n",
        "\n",
        "print(\"\\nPitch Shifted Audio (Lower):\")\n",
        "pitched_down = pitch_shift(waveform, sample_rate, n_steps=-2)\n",
        "visualize_audio(pitched_down, sample_rate, \"Pitch Shifted (Lower)\")\n",
        "\n",
        "# 5. Reverb\n",
        "print(\"\\nReverb Audio:\")\n",
        "reverb = add_reverb(waveform, sample_rate, reverberance=30, damping=50, room_scale=60)\n",
        "visualize_audio(reverb, sample_rate, \"Reverb\")\n",
        "\n",
        "# 6. Filtered Audio\n",
        "print(\"\\nBandpass Filtered Audio:\")\n",
        "filtered = apply_filter(waveform, sample_rate, filter_type='bandpass', cutoff_low=500, cutoff_high=3000)\n",
        "visualize_audio(filtered, sample_rate, \"Bandpass Filtered\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spectrogram Augmentations\n",
        "\n",
        "Now let's visualize how masking augmentations affect the spectrogram representation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute Mel spectrogram for original audio\n",
        "mel_spec = librosa.feature.melspectrogram(y=waveform, sr=sample_rate, n_mels=128)\n",
        "mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "# Apply frequency masking\n",
        "masked_freq = frequency_mask(mel_spec_db, max_mask_width=20, num_masks=2)\n",
        "\n",
        "# Apply time masking\n",
        "masked_time = time_mask(mel_spec_db, max_mask_width=40, num_masks=2)\n",
        "\n",
        "# Apply both\n",
        "masked_both = time_mask(frequency_mask(mel_spec_db, max_mask_width=20, num_masks=2), \n",
        "                        max_mask_width=40, num_masks=2)\n",
        "\n",
        "# Visualize the spectrograms\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "plt.subplot(4, 1, 1)\n",
        "librosa.display.specshow(mel_spec_db, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Original Mel Spectrogram')\n",
        "\n",
        "plt.subplot(4, 1, 2)\n",
        "librosa.display.specshow(masked_freq, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Frequency Masked Mel Spectrogram')\n",
        "\n",
        "plt.subplot(4, 1, 3)\n",
        "librosa.display.specshow(masked_time, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Time Masked Mel Spectrogram')\n",
        "\n",
        "plt.subplot(4, 1, 4)\n",
        "librosa.display.specshow(masked_both, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Time and Frequency Masked Mel Spectrogram')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation with PyTorch Audio Transforms\n",
        "\n",
        "Let's implement the same augmentations using PyTorch audio transforms, which are optimized for GPU processing and integrate well with PyTorch data pipelines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensor\n",
        "waveform_tensor = torch.tensor(waveform, dtype=torch.float32)\n",
        "\n",
        "# Initialize transforms\n",
        "time_shift_transform = torchaudio.transforms.TimeMasking(time_mask_param=int(0.2 * sample_rate))\n",
        "pitch_shift_transform = lambda x: torchaudio.functional.pitch_shift(x.unsqueeze(0), sample_rate, 2).squeeze(0)\n",
        "time_stretch_transform = lambda x: torchaudio.functional.speed(x.unsqueeze(0), 0.8).squeeze(0)\n",
        "\n",
        "# Apply time masking directly to Mel spectrogram\n",
        "mel_spec_transform = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=sample_rate, n_mels=128, n_fft=1024, hop_length=512\n",
        ")\n",
        "time_mask_transform = torchaudio.transforms.TimeMasking(time_mask_param=80)\n",
        "freq_mask_transform = torchaudio.transforms.FrequencyMasking(freq_mask_param=40)\n",
        "\n",
        "# Compute a mel spectrogram for demonstration\n",
        "mel_spec_tensor = mel_spec_transform(waveform_tensor)\n",
        "mel_spec_db_tensor = torchaudio.transforms.AmplitudeToDB()(mel_spec_tensor)\n",
        "\n",
        "# Apply transformations\n",
        "mel_spec_time_masked = time_mask_transform(mel_spec_db_tensor)\n",
        "mel_spec_freq_masked = freq_mask_transform(mel_spec_db_tensor)\n",
        "mel_spec_both_masked = freq_mask_transform(time_mask_transform(mel_spec_db_tensor))\n",
        "\n",
        "# Visualize transformations\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "plt.subplot(4, 1, 1)\n",
        "plt.imshow(mel_spec_db_tensor.numpy()[0], origin='lower', aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.title('Original Mel Spectrogram (PyTorch)')\n",
        "\n",
        "plt.subplot(4, 1, 2)\n",
        "plt.imshow(mel_spec_time_masked.numpy()[0], origin='lower', aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.title('Time Masked Mel Spectrogram (PyTorch)')\n",
        "\n",
        "plt.subplot(4, 1, 3)\n",
        "plt.imshow(mel_spec_freq_masked.numpy()[0], origin='lower', aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.title('Frequency Masked Mel Spectrogram (PyTorch)')\n",
        "\n",
        "plt.subplot(4, 1, 4)\n",
        "plt.imshow(mel_spec_both_masked.numpy()[0], origin='lower', aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.title('Time and Frequency Masked Mel Spectrogram (PyTorch)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating an Augmentation Pipeline for Training\n",
        "\n",
        "Let's create a class that can be used in a PyTorch data pipeline to dynamically augment audio samples during training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AudioAugmenter:\n",
        "    \"\"\"\n",
        "    Apply a series of audio augmentations with configurable probabilities.\n",
        "    \"\"\"\n",
        "    def __init__(self, sample_rate=22050):\n",
        "        self.sample_rate = sample_rate\n",
        "        \n",
        "        # Define augmentation techniques and their probabilities\n",
        "        self.augmentations = {\n",
        "            'time_shift': {'prob': 0.5, 'params': {'shift_pct': 0.2}},\n",
        "            'add_noise': {'prob': 0.3, 'params': {'noise_level': 0.005}},\n",
        "            'pitch_shift': {'prob': 0.4, 'params': {'n_steps': 2}},\n",
        "            'time_stretch': {'prob': 0.4, 'params': {'rate': 1.2}},\n",
        "            'reverb': {'prob': 0.3, 'params': {'reverberance': 30, 'damping': 50, 'room_scale': 50}}\n",
        "        }\n",
        "    \n",
        "    def __call__(self, waveform):\n",
        "        \"\"\"Apply random augmentations to the waveform\"\"\"\n",
        "        # Convert from PyTorch tensor if needed\n",
        "        if isinstance(waveform, torch.Tensor):\n",
        "            is_tensor = True\n",
        "            waveform = waveform.numpy() if waveform.ndim == 1 else waveform[0].numpy()\n",
        "        else:\n",
        "            is_tensor = False\n",
        "        \n",
        "        # Apply augmentations randomly based on their probabilities\n",
        "        for aug_name, aug_config in self.augmentations.items():\n",
        "            if random.random() < aug_config['prob']:\n",
        "                if aug_name == 'time_shift':\n",
        "                    waveform = time_shift(waveform, self.sample_rate, **aug_config['params'])\n",
        "                elif aug_name == 'add_noise':\n",
        "                    waveform = add_noise(waveform, **aug_config['params'])\n",
        "                elif aug_name == 'pitch_shift':\n",
        "                    waveform = pitch_shift(waveform, self.sample_rate, **aug_config['params'])\n",
        "                elif aug_name == 'time_stretch':\n",
        "                    waveform = time_stretch(waveform, **aug_config['params'])\n",
        "                elif aug_name == 'reverb':\n",
        "                    waveform = add_reverb(waveform, self.sample_rate, **aug_config['params'])\n",
        "        \n",
        "        # Convert back to tensor if the input was a tensor\n",
        "        if is_tensor:\n",
        "            waveform = torch.tensor(waveform, dtype=torch.float32)\n",
        "        \n",
        "        return waveform\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demonstration of the Augmentation Pipeline\n",
        "\n",
        "Let's visualize several augmented versions of the same audio file to see the variety we can create.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an augmenter instance\n",
        "augmenter = AudioAugmenter(sample_rate=sample_rate)\n",
        "\n",
        "# Create multiple augmented versions of the same audio\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Original audio\n",
        "plt.subplot(4, 1, 1)\n",
        "librosa.display.waveshow(waveform, sr=sample_rate)\n",
        "plt.title('Original Audio')\n",
        "\n",
        "# Generate three different augmented versions\n",
        "for i in range(3):\n",
        "    augmented = augmenter(waveform)\n",
        "    plt.subplot(4, 1, i+2)\n",
        "    librosa.display.waveshow(augmented, sr=sample_rate)\n",
        "    plt.title(f'Augmented Version {i+1}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Let's save an example spectrogram image of augmented audio for docs\n",
        "mel_spec_augmented = librosa.feature.melspectrogram(y=augmenter(waveform), sr=sample_rate, n_mels=128)\n",
        "mel_spec_db_augmented = librosa.power_to_db(mel_spec_augmented, ref=np.max)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "librosa.display.specshow(mel_spec_db_augmented, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Mel Spectrogram of Augmented Audio')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../docs/images/augmented_spectrogram.png')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integration with PyTorch Dataset\n",
        "\n",
        "Finally, let's see how to integrate our augmentation pipeline into a PyTorch Dataset for easy use during training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AugmentedAudioDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset that applies augmentations to audio samples.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, transform=None, sample_rate=22050, augment=True):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.sample_rate = sample_rate\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        \n",
        "        # Create augmenter if augmentation is enabled\n",
        "        self.augmenter = AudioAugmenter(sample_rate=sample_rate) if augment else None\n",
        "        \n",
        "        # Find all audio files recursively\n",
        "        self.files = []\n",
        "        self.labels = []\n",
        "        \n",
        "        # Here we assume each subfolder name is an emotion label\n",
        "        for emotion_dir in self.root_dir.glob('*'):\n",
        "            if emotion_dir.is_dir():\n",
        "                emotion = emotion_dir.name\n",
        "                for audio_file in emotion_dir.glob('*.wav'):\n",
        "                    self.files.append(audio_file)\n",
        "                    self.labels.append(emotion)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.files[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # Load audio file\n",
        "        waveform, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
        "        \n",
        "        # Apply augmentation if enabled\n",
        "        if self.augment and self.augmenter:\n",
        "            waveform = self.augmenter(waveform)\n",
        "        \n",
        "        # Convert to tensor\n",
        "        waveform = torch.tensor(waveform, dtype=torch.float32)\n",
        "        \n",
        "        # Apply additional transforms if any (e.g., mel spectrogram)\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "        \n",
        "        # Convert label to index (you would need a proper label mapping)\n",
        "        label_idx = {'neutral': 0, 'calm': 1, 'happy': 2, 'sad': 3, \n",
        "                     'angry': 4, 'fearful': 5, 'disgust': 6, 'surprised': 7}.get(label, 0)\n",
        "        \n",
        "        return waveform, label_idx\n",
        "\n",
        "# Example usage (commented out as it requires the dataset)\n",
        "\"\"\"\n",
        "# Define transforms\n",
        "transform = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=22050, n_mels=128, n_fft=1024, hop_length=512\n",
        ")\n",
        "\n",
        "# Create dataset\n",
        "dataset = AugmentedAudioDataset(\n",
        "    root_dir='../processed_dataset/train',\n",
        "    transform=transform,\n",
        "    sample_rate=22050,\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "# Check the dataset\n",
        "if len(dataset) > 0:\n",
        "    # Get a sample\n",
        "    waveform, label = dataset[0]\n",
        "    print(f\"Waveform shape: {waveform.shape}\")\n",
        "    print(f\"Label: {label}\")\n",
        "    \n",
        "    # Create a dataloader\n",
        "    from torch.utils.data import DataLoader\n",
        "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "    \n",
        "    # Get a batch\n",
        "    batch_waveforms, batch_labels = next(iter(dataloader))\n",
        "    print(f\"Batch shape: {batch_waveforms.shape}\")\n",
        "    print(f\"Batch labels: {batch_labels}\")\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we explored various audio augmentation techniques for speech emotion recognition:\n",
        "\n",
        "1. **Time Domain Augmentations**: Time shifting, stretching, adding noise, clipping\n",
        "2. **Frequency Domain Augmentations**: Pitch shifting, frequency masking, time masking\n",
        "3. **Combined Augmentations**: Room reverb simulation, filtering\n",
        "\n",
        "We implemented these techniques using both standard audio processing libraries (librosa) and PyTorch audio transforms, and created a comprehensive augmentation pipeline that can be integrated into a PyTorch dataset.\n",
        "\n",
        "Audio augmentation is especially important for emotion recognition because:\n",
        "\n",
        "- It helps models become robust to variations in speaking rate, pitch, and background noise\n",
        "- It creates a more diverse training set, improving generalization\n",
        "- It mitigates the limited size of emotion datasets, which are often smaller than other speech datasets\n",
        "\n",
        "In the next notebook, we'll explore how these augmented features are processed by our model architectures. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}