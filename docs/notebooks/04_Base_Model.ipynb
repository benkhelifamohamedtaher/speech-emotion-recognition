{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfd7\ufe0f Base Model Architecture (29.7% Accuracy)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook documents the implementation and evaluation of the **Base Model**, our initial approach to speech emotion recognition. This model established our benchmark performance of **29.7% accuracy** on the 8-class emotion classification task.\n",
        "\n",
        "While this performance may seem modest, it represents more than double the random chance accuracy (12.5%) and served as a critical foundation for subsequent architectural improvements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Architecture Overview\n",
        "\n",
        "The Base Model follows a conventional CNN-based architecture for audio processing:\n",
        "\n",
        "```\n",
        "                                     Base Model Architecture\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502               \u2502    \u2502               \u2502    \u2502               \u2502    \u2502               \u2502    \u2502               \u2502\n",
        "\u2502  Audio Input  \u2502\u2500\u2500\u2500\u25ba\u2502  Spectrogram  \u2502\u2500\u2500\u2500\u25ba\u2502  CNN Layers   \u2502\u2500\u2500\u2500\u25ba\u2502  RNN Layers   \u2502\u2500\u2500\u2500\u25ba\u2502  Classifier   \u2502\n",
        "\u2502               \u2502    \u2502  Extraction   \u2502    \u2502  (Feature     \u2502    \u2502  (Temporal    \u2502    \u2502  (Output      \u2502\n",
        "\u2502               \u2502    \u2502               \u2502    \u2502   Extraction) \u2502    \u2502   Modeling)   \u2502    \u2502   Layer)      \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "```\n",
        "\n",
        "### Key Components\n",
        "\n",
        "1. **Audio Preprocessing**: Conversion of raw audio to mel-spectrograms\n",
        "2. **Feature Extraction**: Convolutional layers to detect audio patterns\n",
        "3. **Temporal Modeling**: Simple recurrent layers to capture time dependencies\n",
        "4. **Classification**: Fully-connected layers with softmax activation\n",
        "\n",
        "Let's examine each component in detail.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Audio Preprocessing\n",
        "\n",
        "Audio preprocessing is a critical step in speech emotion recognition. Raw audio waveforms are converted to mel-spectrograms, which represent the frequency content of the signal over time in a way that approximates human auditory perception.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def extract_features(file_path, sample_rate=22050, n_mels=128, n_fft=2048, hop_length=512):\n",
        "    \"\"\"\n",
        "    Extract mel-spectrogram features from an audio file\n",
        "    \n",
        "    Parameters:\n",
        "        file_path (str): Path to the audio file\n",
        "        sample_rate (int): Sample rate for loading the audio\n",
        "        n_mels (int): Number of mel bands\n",
        "        n_fft (int): FFT window size\n",
        "        hop_length (int): Hop length for STFT\n",
        "        \n",
        "    Returns:\n",
        "        mel_spectrogram (np.ndarray): Mel-spectrogram features\n",
        "    \"\"\"\n",
        "    # Load audio file\n",
        "    y, sr = librosa.load(file_path, sr=sample_rate)\n",
        "    \n",
        "    # Extract mel-spectrogram\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(\n",
        "        y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length\n",
        "    )\n",
        "    \n",
        "    # Convert to decibels\n",
        "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "    \n",
        "    return mel_spectrogram_db\n",
        "\n",
        "# Example: Display mel-spectrogram for a sample audio file\n",
        "# file_path = \"../samples/neutral_sample.wav\"  # Update with actual path\n",
        "# mel_spec = extract_features(file_path)\n",
        "#\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# librosa.display.specshow(mel_spec, x_axis='time', y_axis='mel', sr=22050, hop_length=512)\n",
        "# plt.colorbar(format='%+2.0f dB')\n",
        "# plt.title('Mel-spectrogram')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Base Model Architecture\n",
        "\n",
        "Our Base Model uses a combination of convolutional and recurrent layers implemented in PyTorch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BaseEmotionModel(nn.Module):\n",
        "    def __init__(self, num_emotions=8):\n",
        "        \"\"\"\n",
        "        Initialize the base emotion recognition model\n",
        "        \n",
        "        Parameters:\n",
        "            num_emotions (int): Number of emotion classes\n",
        "        \"\"\"\n",
        "        super(BaseEmotionModel, self).__init__()\n",
        "        \n",
        "        # Convolutional layers for feature extraction\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # Recurrent layer for temporal modeling\n",
        "        self.gru = nn.GRU(input_size=128 * 16, hidden_size=128, num_layers=2, batch_first=True)\n",
        "        \n",
        "        # Fully connected layers for classification\n",
        "        self.fc1 = nn.Linear(128, 64)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(64, num_emotions)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network\n",
        "        \n",
        "        Parameters:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, 1, height, width)\n",
        "                             representing mel-spectrograms\n",
        "                             \n",
        "        Returns:\n",
        "            torch.Tensor: Logits for each emotion class\n",
        "        \"\"\"\n",
        "        # CNN feature extraction\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Reshape for GRU\n",
        "        batch_size, channels, height, width = x.size()\n",
        "        x = x.permute(0, 3, 1, 2)  # (batch, width, channels, height)\n",
        "        x = x.reshape(batch_size, width, channels * height)\n",
        "        \n",
        "        # RNN temporal modeling\n",
        "        x, _ = self.gru(x)\n",
        "        x = x[:, -1, :]  # Take the output from the last time step\n",
        "        \n",
        "        # Classification\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Create model instance\n",
        "base_model = BaseEmotionModel(num_emotions=8)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in base_model.parameters() if p.requires_grad):,}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training Process\n",
        "\n",
        "The Base Model was trained using the following approach:\n",
        "\n",
        "- **Optimizer**: Adam with learning rate of 1e-4\n",
        "- **Loss Function**: Cross-Entropy Loss\n",
        "- **Batch Size**: 32\n",
        "- **Epochs**: 50\n",
        "- **Early Stopping**: Patience of 10 epochs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_base_model(model, train_loader, val_loader, num_epochs=50, learning_rate=1e-4):\n",
        "    \"\"\"\n",
        "    Train the base emotion recognition model\n",
        "    \n",
        "    Parameters:\n",
        "        model (nn.Module): The model to train\n",
        "        train_loader (DataLoader): DataLoader for training data\n",
        "        val_loader (DataLoader): DataLoader for validation data\n",
        "        num_epochs (int): Number of training epochs\n",
        "        learning_rate (float): Learning rate for optimizer\n",
        "        \n",
        "    Returns:\n",
        "        dict: Training history (losses and accuracies)\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Initialize optimizer and loss function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "    \n",
        "    # Early stopping parameters\n",
        "    best_val_acc = 0\n",
        "    patience = 10\n",
        "    patience_counter = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        \n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Statistics\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += targets.size(0)\n",
        "            train_correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_acc = train_correct / train_total\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                \n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                \n",
        "                # Statistics\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += targets.size(0)\n",
        "                val_correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = val_correct / val_total\n",
        "        \n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        \n",
        "        # Print statistics\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | \"\n",
        "              f\"Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | \"\n",
        "              f\"Val Acc: {val_acc:.4f}\")\n",
        "        \n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), \"base_model_best.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "    \n",
        "    return history\n",
        "\n",
        "# Note: Actual training would be executed here with prepared DataLoaders\n",
        "# history = train_base_model(base_model, train_loader, val_loader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Results Analysis\n",
        "\n",
        "The Base Model achieved an accuracy of **29.7%** on the validation set, which is our benchmark performance. Let's analyze the confusion matrix and classification metrics to understand the model's strengths and weaknesses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set\n",
        "    \n",
        "    Parameters:\n",
        "        model (nn.Module): The trained model\n",
        "        test_loader (DataLoader): DataLoader for test data\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (all_predictions, all_targets) for further analysis\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "    \n",
        "    return np.array(all_predictions), np.array(all_targets)\n",
        "\n",
        "# Example: Display results\n",
        "# class_names = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
        "# predictions, targets = evaluate_model(base_model, test_loader)\n",
        "# \n",
        "# # Confusion matrix\n",
        "# cm = confusion_matrix(targets, predictions)\n",
        "# cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "# \n",
        "# plt.figure(figsize=(10, 8))\n",
        "# sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "#             xticklabels=class_names, yticklabels=class_names)\n",
        "# plt.xlabel('Predicted')\n",
        "# plt.ylabel('True')\n",
        "# plt.title('Confusion Matrix - Base Model (29.7%)')\n",
        "# plt.tight_layout()\n",
        "# \n",
        "# # Classification report\n",
        "# report = classification_report(targets, predictions, target_names=class_names, output_dict=True)\n",
        "# report_df = pd.DataFrame(report).transpose()\n",
        "# display(report_df.round(3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Base Model Strengths and Limitations\n",
        "\n",
        "### Strengths\n",
        "\n",
        "1. **Simplicity**: The model architecture is straightforward and efficient to train.\n",
        "2. **Baseline Performance**: Establishes a solid baseline of 29.7% accuracy (more than 2x random chance).\n",
        "3. **Fast Training**: Completes training in approximately 2 hours on standard hardware.\n",
        "4. **Low Parameters**: The model has fewer parameters compared to more complex architectures, making it memory-efficient.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "1. **Limited Temporal Modeling**: The simple GRU layers don't fully capture the complex temporal relationships in emotional speech.\n",
        "2. **Feature Extraction Depth**: Shallow convolutional layers may not extract sufficiently discriminative features.\n",
        "3. **Context Awareness**: The model lacks attention mechanisms to focus on the most emotionally salient parts of speech.\n",
        "4. **Confusion Among Similar Emotions**: High confusion rates between similar emotion pairs (e.g., neutral/calm).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Key Learnings and Next Steps\n",
        "\n",
        "From the Base Model implementation, we gained several insights that informed the development of subsequent models:\n",
        "\n",
        "1. **Need for Better Feature Extraction**: The Enhanced Model should incorporate deeper convolutional layers.\n",
        "2. **Importance of Attention**: Adding attention mechanisms could help focus on emotionally relevant audio segments.\n",
        "3. **Temporal Modeling Improvement**: More sophisticated recurrent structures could better capture emotion dynamics.\n",
        "4. **Data Augmentation**: Techniques like time stretching and pitch shifting might improve generalization.\n",
        "\n",
        "In the next notebook, we'll explore the Enhanced Model (31.5% accuracy), which addresses some of these limitations through attention mechanisms and improved feature extraction. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}